{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "---------\n",
    "\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-05-17  \n",
    "**Last Updated:** 2023-05-22  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "This notebook should serve to educate newcomers to Python on simple Machine Learning techniques in the context of Record Linkage.\n",
    "More specifically, we'll be creating a decision tree using very basic parameter tuning.\n",
    "We will visualize our output, measure some basic metrics, and save the model.\n",
    "\n",
    "### Notes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prep Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read in data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Synthetic Data for Person1\n",
    "\n",
    "Synthetic string-type data for person 1.\n",
    "This data is used at the end of the script for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic1 = pd.read_csv('Data/synthetic_df1.csv', dtype=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Synthetic Data for Person2\n",
    "\n",
    "Synthetic string-type data for person 2.\n",
    "This data is used at the end of the script for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic2 = pd.read_csv('Data/synthetic_df2.csv', dtype=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Training Data\n",
    "\n",
    "This data is used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv('Data/synthetic_training_data.csv',index_col=[0,1])\n",
    "df_training.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually pick out example of two people and how they were compared\n",
    "row1 = df_synthetic1[['fname','lname','dob','phone','add']].loc[78604:78604]\n",
    "row2 = df_synthetic2[['fname','lname','dob','phone','add']].loc[100214:100214]\n",
    "row3 = df_training.sort_index().loc[(78604,100214):(78604,100214)].drop('label',axis=1)\n",
    "\n",
    "# Format our output\n",
    "df_comparisons = pd.concat([row1,row2,row3],ignore_index=True)\n",
    "df_comparisons.index = ['person1','person2','numeric_comparisons']\n",
    "df_comparisons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_training.drop('label',axis=1), df_training['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "\n",
    "#### Notes on parameter tuning\n",
    "----- \n",
    "\n",
    "Parameter tuning in machine learning is the process of selecting the appropriate set of optimal parameters for a model in order to improve its accuracy or other metrics. In the case of the Decision Tree, the parameters we are tuning are 'max_depth' and 'criterion'.\n",
    "\n",
    "'max_depth' is the maximum depth of the tree. It is used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "'criterion' is the function to measure the quality of a split. Sklearn supports “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.\n",
    "\n",
    "In our code, we used GridSearchCV with 5-fold cross-validation for the hyperparameter tuning of our Decision Tree model. This means the training set is split into 5 parts and the model will be trained and validated 5 times to get a better estimate of the model performance.\n",
    "\n",
    "After identifying the best parameters using GridSearchCV, the model is evaluated on the test data. Evaluation metrics like the classification report and confusion matrix provide comprehensive insights about model performance. The classification report shows precision, recall, f1-score, and support for each class while the confusion matrix visualizes the correct predictions and the errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {'max_depth': range(1, 5), 'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# Initialize a DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy score\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Decision Tree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming grid.best_estimator_ is your trained DecisionTreeClassifier\n",
    "dt_best = grid.best_estimator_\n",
    "\n",
    "# Set larger figure size\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plot the tree\n",
    "plot_tree(dt_best, filled=True, rounded=True, feature_names=X_train.columns, class_names=True, proportion=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Manually inspect wrong decisions\n",
    "\n",
    "This chunk identifies the misclassified examples from a machine learning model's predictions and prints out some examples of these incorrectly classified instances\n",
    "It's a good way to analyze where the model is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the predicted values (y_pred) with the actual test values (y_test)\n",
    "# and store the boolean result in list_results (True where prediction is correct, False where incorrect)\n",
    "list_results = y_pred == y_test\n",
    "\n",
    "# Extract the indices where the predicted values didn't match the actual test values\n",
    "wrong_guess_indices = list_results[list_results == False].index\n",
    "\n",
    "# Use these indices to get the corresponding rows from the training data\n",
    "wrong_responses = df_training.loc[wrong_guess_indices]\n",
    "\n",
    "# Split the incorrectly predicted data into two parts: one where the actual label is 0 and one where the actual label is 1\n",
    "wrong_responses_label0 = wrong_responses.query('label == 0')\n",
    "wrong_responses_label1 = wrong_responses.query('label == 1')\n",
    "\n",
    "# List of columns that are used in the analysis\n",
    "used_cols = ['ssn','fname','lname','dob','phone','add']\n",
    "\n",
    "# Number of examples to print from each class\n",
    "num_examples_printed_each_class = 2\n",
    "\n",
    "# Loop through the number of examples to be printed\n",
    "for i in range(0,num_examples_printed_each_class):\n",
    "    # Get the indices for the original dataframes (df_synthetic1 and df_synthetic2) for the misclassified examples where actual label is 0\n",
    "    df1_index = wrong_responses_label0.iloc[i].name[0]\n",
    "    df2_index = wrong_responses_label0.iloc[i].name[1]\n",
    "\n",
    "    # Use these indices to fetch the corresponding rows from the original dataframes\n",
    "    part1 = df_synthetic1.loc[int(df1_index)][used_cols]\n",
    "    part2 = df_synthetic2.loc[int(df2_index)][used_cols]\n",
    "\n",
    "    # Combine the two parts and transpose to get a dataframe with the used_cols as columns\n",
    "    combined = pd.concat([part1,part2], ignore_index=True, axis=1).transpose()\n",
    "\n",
    "    # Print and display the misclassified examples where actual label is 0 and model predicted 1\n",
    "    print('label = 0')\n",
    "    print('model decision = 1')\n",
    "    display(combined)\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Similar loop for the misclassified examples where the actual label is 1\n",
    "for i in range(0,num_examples_printed_each_class):\n",
    "    # Get the indices for the original dataframes (df_synthetic1 and df_synthetic2) for the misclassified examples where actual label is 1\n",
    "    df1_index = wrong_responses_label1.iloc[i].name[0]\n",
    "    df2_index = wrong_responses_label1.iloc[i].name[1]\n",
    "\n",
    "    # Use these indices to fetch the corresponding rows from the original dataframes\n",
    "    part1 = df_synthetic1.loc[int(df1_index)][used_cols]\n",
    "    part2 = df_synthetic2.loc[int(df2_index)][used_cols]\n",
    "\n",
    "    # Combine the two parts and transpose to get a dataframe with the used_cols as columns\n",
    "    combined = pd.concat([part1,part2], ignore_index=True, axis=1).transpose()\n",
    "\n",
    "    # Print and display the misclassified examples where actual label is 1 and model predicted 0\n",
    "    print('label = 1')\n",
    "    print('model decision = 0')\n",
    "    display(combined)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
