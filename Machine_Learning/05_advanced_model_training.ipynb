{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model Training\n",
    "---------\n",
    "\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-05-23  \n",
    "**Last Updated:** 2023-05-23  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "This notebook should serve to show a more complex model training with features such as:\n",
    "- mlflow monitoring\n",
    "- neural network model\n",
    "\n",
    "### Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Supporting libs\n",
    "import cloudpickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# MLFlow specific libs\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Sci-kit learn specific libs\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prep Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/advanced_synthetic_training_data.csv',index_col = [0,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label',axis=1), df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML Model\n",
    "\n",
    "### 3.1 Define Param Grid Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the possible sizes for the hidden layers in the neural network. Each tuple represents a \n",
    "# different network architecture. For example, (100,) means one hidden layer with 100 neurons,\n",
    "# (50,2) means two hidden layers with 50 neurons each, etc.\n",
    "hidden_layer_sizes = [(100,), (50,2), (100,2), (50,3),(10,5)]\n",
    "\n",
    "# Define the possible activation functions for the neurons in the network. 'tanh' and 'relu' are\n",
    "# two common choices.\n",
    "activation = ['tanh', 'relu']\n",
    "\n",
    "# Define the possible solvers for weight optimization in the network. 'sgd' stands for Stochastic \n",
    "# Gradient Descent, and 'adam' is a method that computes adaptive learning rates for each weight.\n",
    "solver = ['sgd', 'adam']\n",
    "\n",
    "# Define the possible values for alpha, the regularization parameter in the MLPClassifier. This\n",
    "# parameter helps prevent overfitting by constraining the size of the weights.\n",
    "alpha = [0.0001, 0.05]\n",
    "\n",
    "# Define the possible learning rate schedules for weight updates. 'constant' means the learning \n",
    "# rate stays the same throughout training, and 'adaptive' means the learning rate decreases \n",
    "# whenever progress on the training set stalls.\n",
    "learning_rate = ['constant','adaptive']\n",
    "\n",
    "# Consolidate all these lists into one list, which forms a grid of hyperparameters to be explored.\n",
    "# This will be used later to randomly select a set of hyperparameters for each run of the model.\n",
    "param_grid = [ hidden_layer_sizes , activation, solver, alpha, learning_rate ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Begin MLFlow runs\n",
    "\n",
    "This might look complex with the class and all, but it is far from that.\n",
    "The only reason we set up the class is so that we can use a sklearn model in our mlflow log and output a prediction as a probability, not a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper class for sklearn models that implements the mlflow.pyfunc.PythonModel interface.\n",
    "# This allows the model to be used with MLflow's pyfunc model flavor, which allows it to be loaded\n",
    "# as a Python function for inference.\n",
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model):\n",
    "    # Store the model object (fitted sklearn model).\n",
    "    self.model = model\n",
    "\n",
    "  # Define the predict method that the model will use for inference.  \n",
    "  def predict(self, context, model_input):\n",
    "    # For classification problems, sklearn's predict_proba method returns a 2D array with one row\n",
    "    # per input and one column per class. The column at index 1 represents the probability of the\n",
    "    # positive class, so we select it with [:,1].\n",
    "    return self.model.predict_proba(model_input)[:,1]\n",
    "\n",
    "# Enable automatic logging of hyperparameters, metrics, and the trained model.\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Loop over a range of values from 1 to 50.\n",
    "for i in np.arange(1,50):\n",
    "  \n",
    "  cur_run_params = []\n",
    "\n",
    "  # For each parameter in the parameter grid, select a random value and add it to the current\n",
    "  # run's parameters.\n",
    "  for j in np.arange(0,len(param_grid)):\n",
    "    cur_run_params.append(random.choice(param_grid[j]))\n",
    "\n",
    "  # Start a new run in MLflow, giving it a specific name.\n",
    "  with mlflow.start_run(run_name='RecordLinkage_NeuralNetwork'):\n",
    "\n",
    "    # Initialize the MLPClassifier model with the current run's parameters.\n",
    "    model = MLPClassifier(hidden_layer_sizes =  cur_run_params[0],\n",
    "                          activation =  cur_run_params[1],\n",
    "                          solver =  cur_run_params[2],\n",
    "                          alpha =  cur_run_params[3],\n",
    "                          learning_rate =  cur_run_params[4])\n",
    "\n",
    "    # Train the model and log the time it took to train.\n",
    "    fit_start = time.time()\n",
    "    model.fit(X_train,y_train)\n",
    "    fit_end = time.time()\n",
    "    mlflow.log_metric('fit_time', fit_end - fit_start) \n",
    "\n",
    "    # Predict on the test set and log the time it took to predict.\n",
    "    predict_start = time.time()\n",
    "    pred_proba_test = model.predict_proba(X_test)[:,1]\n",
    "    predict_end = time.time()\n",
    "    mlflow.log_metric('predict_time', predict_end - predict_start)\n",
    "\n",
    "    # Log various performance metrics.\n",
    "    test_auc_score = roc_auc_score(y_test, pred_proba_test)\n",
    "    mlflow.log_metric('auc', test_auc_score)\n",
    "    predict_binary_test = model.predict(X_test)\n",
    "    test_f1_score = f1_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('f1', test_f1_score)\n",
    "    test_accuracy = accuracy_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('accuracy', test_accuracy)\n",
    "    test_precision = precision_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('precision', test_precision)\n",
    "    test_recall = recall_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('recall', test_recall)\n",
    "\n",
    "    # Wrap the trained model using the SklearnModelWrapper class defined earlier.\n",
    "    wrappedModel = SklearnModelWrapper(model)\n",
    "\n",
    "    # Infer the signature of the model. The signature defines the input and output schema of the\n",
    "    # model. When the model is deployed, this signature will be used to validate inputs.\n",
    "    signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n",
    "\n",
    "    # Log the model. This includes the model object, the wrapper class, and the inferred signature\n",
    "    mlflow.pyfunc.log_model(\"RecordLinkage_NeuralNetwork\", python_model=wrappedModel, signature=signature)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manually inspect output\n",
    "\n",
    "Open up a command prompt.  I'm using visual studio code, so this it is as easy as clicking the `Terminal` dropdown in the top menu, selecting new terminal, and then opening a command prompt there.\n",
    "\n",
    "From that point, ensure that you are working in the proper virtual environment.\n",
    "Should look something like this command:\n",
    "\n",
    "```cmd\n",
    "virtual_environment_name\\Scripts\\activate\n",
    "```\n",
    "\n",
    "From that point, navigate to the MachineLearning directory:\n",
    "\n",
    "```cmd\n",
    "cd MachineLearning\n",
    "```\n",
    "\n",
    "Then, you can open the MLFlow user interface by typing:\n",
    "\n",
    "```cmd\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "It should buffer for a second, and then show you an instance of a locally hosted server (website thing) that you can `CTRL+click` on to compare MLFlow runs.\n",
    "This tool is very useful for choosing a model.\n",
    "I like to pick a model with high precision (low number of False Positives), high accuracy (usually right), high AUC (low error margins in predicted probability), and low predict time (fast model).  In general the high precision is likely the most important for record linkage, because false positives can lead to future transitive links, which causes a metaphorical cascade of issues.  False positives are more costly than false negatives in this context.\n",
    "\n",
    "-----------\n",
    "\n",
    "Once you choose a model you like, you can register it and transition to staging or production.  This way, you can track models and versions.\n",
    "As you diagnose issues with your model, you can modify your training data to improve upon it and train better models.\n",
    "Go MLOps!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continued Hyperparameter Tuning\n",
    "\n",
    "I found that solver='adam' and activation='relu' were generally associated with better performing models.\n",
    "Holding these two constant, we'll adjust some others."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the possible sizes for the hidden layers in the neural network. Each tuple represents a \n",
    "# different network architecture. For example, (50,2) means two hidden layers with 50 neurons each,\n",
    "# (20,5) means two hidden layers with 20 and 5 neurons respectively, etc.\n",
    "hidden_layer_sizes = [(50,2), (20,5), (10,10)]\n",
    "\n",
    "# Define the possible values for alpha, the regularization parameter in the MLPClassifier. This\n",
    "# parameter helps prevent overfitting by constraining the size of the weights.\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "# Define the possible learning rate schedules for weight updates. 'constant' means the learning \n",
    "# rate stays the same throughout training, 'invscaling' decreases the learning rate gradually, and \n",
    "# 'adaptive' means the learning rate decreases whenever progress on the training set stalls.\n",
    "learning_rate = ['constant', 'invscaling', 'adaptive']\n",
    "\n",
    "# Define the learning_rate_init, which sets the initial learning rate for weight updates. This can be\n",
    "# a critical hyperparameter, as a learning rate that's too high can cause the model to converge too\n",
    "# quickly to a suboptimal solution, while a learning rate that's too low can cause the model to get\n",
    "# stuck or to progress very slowly.\n",
    "learning_rate_init = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Define the max_iter, which is the maximum number of iterations for the solver to converge. This\n",
    "# hyperparameter can affect both the training time and the quality of the model. Too few iterations\n",
    "# might mean the model hasn't fully converged, while too many might lead to overfitting.\n",
    "max_iter = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Define the tol, which is the tolerance for the optimization. When the loss or score is not improving \n",
    "# by at least tol for n_iter_no_change consecutive iterations, the convergence is considered to be \n",
    "# sufficient and training stops.\n",
    "tol = [0.0001, 0.001, 0.01]\n",
    "\n",
    "# Consolidate all these lists into one list, which forms a grid of hyperparameters to be explored.\n",
    "# This will be used later to randomly select a set of hyperparameters for each run of the model.\n",
    "param_grid = [hidden_layer_sizes, alpha, learning_rate, learning_rate_init, max_iter, tol]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper class for sklearn models that implements the mlflow.pyfunc.PythonModel interface.\n",
    "# This allows the model to be used with MLflow's pyfunc model flavor, which allows it to be loaded\n",
    "# as a Python function for inference.\n",
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model):\n",
    "    # Store the model object (fitted sklearn model).\n",
    "    self.model = model\n",
    "\n",
    "  # Define the predict method that the model will use for inference.  \n",
    "  def predict(self, context, model_input):\n",
    "    # For classification problems, sklearn's predict_proba method returns a 2D array with one row\n",
    "    # per input and one column per class. The column at index 1 represents the probability of the\n",
    "    # positive class, so we select it with [:,1].\n",
    "    return self.model.predict_proba(model_input)[:,1]\n",
    "\n",
    "# Enable automatic logging of hyperparameters, metrics, and the trained model.\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Loop over a range of values from 1 to 50.\n",
    "for i in np.arange(1,101):\n",
    "  \n",
    "  cur_run_params = []\n",
    "\n",
    "  # For each parameter in the parameter grid, select a random value and add it to the current\n",
    "  # run's parameters.\n",
    "  for j in np.arange(0,len(param_grid)):\n",
    "    cur_run_params.append(random.choice(param_grid[j]))\n",
    "\n",
    "  # Start a new run in MLflow, giving it a specific name.\n",
    "  with mlflow.start_run(run_name='RecordLinkage_NeuralNetwork_V2'):\n",
    "\n",
    "    # Initialize the MLPClassifier model with the current run's parameters.\n",
    "    model = MLPClassifier(hidden_layer_sizes =  cur_run_params[0],\n",
    "                          alpha =  cur_run_params[1],\n",
    "                          learning_rate =  cur_run_params[2],\n",
    "                          learning_rate_init  =  cur_run_params[3],\n",
    "                          max_iter =  cur_run_params[4],\n",
    "                          tol = cur_run_params[5],\n",
    "                          activation = 'relu',\n",
    "                          solver = 'adam')\n",
    "\n",
    "    # Train the model and log the time it took to train.\n",
    "    fit_start = time.time()\n",
    "    model.fit(X_train,y_train)\n",
    "    fit_end = time.time()\n",
    "    mlflow.log_metric('fit_time', fit_end - fit_start) \n",
    "\n",
    "    # Predict on the test set and log the time it took to predict.\n",
    "    predict_start = time.time()\n",
    "    pred_proba_test = model.predict_proba(X_test)[:,1]\n",
    "    predict_end = time.time()\n",
    "    mlflow.log_metric('predict_time', predict_end - predict_start)\n",
    "\n",
    "    # Log various performance metrics.\n",
    "    test_auc_score = roc_auc_score(y_test, pred_proba_test)\n",
    "    mlflow.log_metric('auc', test_auc_score)\n",
    "    predict_binary_test = model.predict(X_test)\n",
    "    test_f1_score = f1_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('f1', test_f1_score)\n",
    "    test_accuracy = accuracy_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('accuracy', test_accuracy)\n",
    "    test_precision = precision_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('precision', test_precision)\n",
    "    test_recall = recall_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('recall', test_recall)\n",
    "\n",
    "    # Wrap the trained model using the SklearnModelWrapper class defined earlier.\n",
    "    wrappedModel = SklearnModelWrapper(model)\n",
    "\n",
    "    # Infer the signature of the model. The signature defines the input and output schema of the\n",
    "    # model. When the model is deployed, this signature will be used to validate inputs.\n",
    "    signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n",
    "\n",
    "    # Log the model. This includes the model object, the wrapper class, and the inferred signature\n",
    "    mlflow.pyfunc.log_model(\"RecordLinkage_NeuralNetwork_V2\", python_model=wrappedModel, signature=signature)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
