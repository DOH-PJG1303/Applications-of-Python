{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex training set\n",
    "---------\n",
    "\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-05-23  \n",
    "**Last Updated:** 2023-05-23  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "This notebook should serve to show a more complex record linkage process with new features such as:\n",
    "- advanced preprocessing\n",
    "- customized RL comparison functions\n",
    "\n",
    "### Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Phonetic encoding library\n",
    "import jellyfish\n",
    "\n",
    "# Record linkage specific resources\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean, phonetic\n",
    "from recordlinkage.index import Block\n",
    "from recordlinkage.base import BaseCompareFeature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Data\n",
    "\n",
    "Here, we're using synthetic data that I created to replicate Oregon's population.\n",
    "The data represents the population of individuals in the years 2020-2022 who were born in Coos County in Oregon.\n",
    "They may live elsewhere.\n",
    "Interesting features that this training data includes:\n",
    "* several people can live in the same building\n",
    "* families exist.  If a child is <18, they live with their parent or parents\n",
    "* fields change dependent on the year. If someone gets married and changes their name in 2020, their data in 2019 will look different than their data in 2020 for the lname field.\n",
    "\n",
    "For more information, reach out to PJ and he'd be happy to elaborate.\n",
    "Again, this data is synthetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Data/synthetic_df1.csv', dtype=str)\n",
    "df2 = pd.read_csv('Data/synthetic_df2.csv', dtype=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Record Linkage Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Function to apply a series of transformations to a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        The DataFrame to which the transformations should be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        The DataFrame after applying the transformations.\n",
    "    \"\"\"\n",
    "    # Lowercase and remove non-alphabetical characters in 'fname' and 'lname'\n",
    "    for field in ['fname', 'lname']:\n",
    "        df[field] = df[field].str.lower().str.replace('[^a-z]','',regex=True)\n",
    "\n",
    "    # Lowercase and take the first character of 'mname'\n",
    "    df['mname'] = df['mname'].str.lower().str.slice(1,2)\n",
    "\n",
    "    # Fix the format of 'dob' to ensure two-digit days and months\n",
    "    df['dob'] = df['dob'].str.replace('-(\\d)(?=-|$)','-0\\\\1',regex=True)\n",
    "\n",
    "    # Remove non-digits from 'phone', remove leading 0 or 1, and remove any digit repeated more than 6 times consecutively\n",
    "    df['phone'] = df['phone'].str.replace('\\D','',regex=True).str.replace('^[01]','',regex=True).str.replace( '.*(\\d)\\\\1{6,}.*', '',regex=True)\n",
    "\n",
    "    # Lowercase and remove non-alphabetical and non-digital characters (excluding @ and .) in 'email'\n",
    "    df['email'] = df['email'].str.lower().str.replace('[^a-z0-9\\.@]','', regex=True)\n",
    "\n",
    "    # Lowercase, remove non-alphabetical and non-digital characters in 'add', and replace common street abbreviations and directions\n",
    "    df['add'] = df['add'].str.lower().str.replace('[^a-z0-9 ]','',regex=True)\n",
    "    replacements = {\n",
    "        ' street': ' st',\n",
    "        ' avenue': ' ave',\n",
    "        ' road': ' rd',\n",
    "        ' place': ' pl',\n",
    "        ' drive': ' dr',\n",
    "        ' court': ' ct',\n",
    "        ' lane': ' ln',\n",
    "        ' boulevard': ' blvd',\n",
    "        ' highway': ' hwy',\n",
    "        ' circle': ' cir',\n",
    "        ' apartment': ' apt',\n",
    "        ' suite': ' ste',\n",
    "        ' north': ' n',\n",
    "        ' south': ' s',\n",
    "        ' east': ' e',\n",
    "        ' west': ' w',\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        df['add'] = df['add'].str.replace(f'(?<!(\\S)){k}(?=(\\s|$))', v, regex=True)\n",
    "\n",
    "    # Remove non-digits from 'zip'\n",
    "    df['zip'] = df['zip'].str.replace('\\D','',regex=True)\n",
    "\n",
    "    # Lowercase, remove non-alphabetical characters and ' county' in 'county'\n",
    "    df['county'] = df['county'].str.lower().str.replace('[^a-z ]','', regex=True).str.replace(' county','', regex=True)\n",
    "\n",
    "    # Replace empty fields or fields with placeholder values with NaN\n",
    "    fields = ['fname','mname','lname','dob','phone','email','add','zip','county']\n",
    "    for field in fields:\n",
    "        df[field] = df[field].apply(lambda x: np.nan if ((len(str(x).strip()) == 0)|(str(x).strip() in ['none','na','missing','unknown'])) else x)\n",
    "\n",
    "\n",
    "    # Generate metaphone versions of the fields\n",
    "    for col in ['fname', 'lname']:\n",
    "        df['meta_'+col] = phonetic(df1[col], method='metaphone')\n",
    "        df['sdx_'+col] = phonetic(df2[col], method='soundex')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = apply_preprocessing(df1)\n",
    "df2 = apply_preprocessing(df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index (pairs of records to compare)\n",
    "indexer = rl.Index()\n",
    "\n",
    "# Generate a blocking scheme as a union of the following blocks\n",
    "indexer.add(Block('dob'))\n",
    "indexer.add(Block(['meta_fname', 'meta_lname']))\n",
    "indexer.add(Block('building_id'))\n",
    "indexer.add(Block('parents_partnership_id'))\n",
    "\n",
    "pairs = indexer.index(df1, df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_dob_score(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, dob1, dob2):\n",
    "        \n",
    "        # Initialize an empty pandas series to hold the comparison scores\n",
    "        score = pd.Series(np.nan, index=dob1.index)\n",
    "\n",
    "        # If either date is missing, return 0\n",
    "        score[(dob1.isnull()) | (dob2.isnull())] = -1\n",
    "\n",
    "        # Extract the year, month, and day from each date\n",
    "        dob_y_1 = dob1.str[2:4]\n",
    "        dob_y_2 = dob2.str[2:4]\n",
    "\n",
    "        dob_m_1 = dob1.str[5:7]\n",
    "        dob_m_2 = dob2.str[5:7]\n",
    "\n",
    "        dob_d_1 = dob1.str[8:]\n",
    "        dob_d_2 = dob2.str[8:]\n",
    "\n",
    "        # Check whether the year, month, and day are the same for each date\n",
    "        same_y = (dob_y_1 == dob_y_2)\n",
    "        same_m = (dob_m_1 == dob_m_2)\n",
    "        same_d = (dob_d_1 == dob_d_2)\n",
    "\n",
    "        # Check whether the month and day are swapped between the two dates\n",
    "        swap_m_d = (dob_m_1 == dob_d_2) & (dob_d_1 == dob_m_2)\n",
    "        swap_check = (dob_m_1 != dob_m_2)\n",
    "\n",
    "        # If the year is the same and the month and day are swapped, return 2.5/3.0\n",
    "        score[(same_y & swap_m_d & swap_check)] = 2.5/3.0\n",
    "\n",
    "        # Otherwise, return the average of whether the year, month, and day are the same\n",
    "        score[(~same_y | ~swap_m_d | ~swap_check)] = (same_y + same_m + same_d).astype(int) / 3.0\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_name_score(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, raw1, sdx1, meta1, raw2, sdx2, meta2):\n",
    "        \n",
    "        # Initialize an empty pandas series to hold the comparison scores\n",
    "        score = pd.Series(np.nan, index=raw1.index)\n",
    "\n",
    "        # If any of the data is missing, return -1.0\n",
    "        score[(raw1.isnull()) | (sdx1.isnull()) | (meta1.isnull()) | (raw2.isnull()) | (sdx2.isnull()) | (meta2.isnull())] = -1\n",
    "\n",
    "        # Check if raw1 is in raw2 or vice versa\n",
    "        out = raw1.combine(raw2, lambda x, y: x in y or y in x if isinstance(x, str) and isinstance(y, str) else False).astype(int)\n",
    "\n",
    "        # Check if sdx1 is equal to sdx2\n",
    "        out += (sdx1 == sdx2).astype(int)\n",
    "\n",
    "        # Check if meta1 is equal to meta2\n",
    "        out += (meta1 == meta2).astype(int)\n",
    "\n",
    "        # Store the average score in the 'score' series\n",
    "        score[~((raw1.isnull()) | (sdx1.isnull()) | (meta1.isnull()) | (raw2.isnull()) | (sdx2.isnull()) | (meta2.isnull()))] = out / 4.0\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_custom_dlev_distance(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, s1, s2):\n",
    "        \"\"\"\n",
    "        This function calculates the Damerau-Levenshtein distance between two strings s1 and s2.\n",
    "        It normalizes the distance by the average length of the two strings.\n",
    "        If either string is None, it returns -1.0.\n",
    "        \"\"\"\n",
    "        out = [-1.0 if pd.isnull(x) or pd.isnull(y) \n",
    "               else 1.0 - (jellyfish.damerau_levenshtein_distance(x, y) / ((len(x) + len(y)) / 2.0)) \n",
    "               for x, y in zip(s1, s2)]\n",
    "        return pd.Series(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_swapped_fields(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, s1_1, s1_2, s2_1, s2_2):\n",
    "        \"\"\"\n",
    "        This function determines whether or not someone accidentally swapped two fields\n",
    "        \"\"\"\n",
    "        return ((s1_1 == s2_2) & (s1_2 == s2_1) ).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Define Comparisons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|    | Field1   | Field2   | Method    |\n",
    "|---:|:---------|:---------|:----------|\n",
    "|  0 | fname_1  | fname_2  | jaro_wink |\n",
    "|  1 | fname_1  | fname_2  | dlev_len  |\n",
    "|  2 | mname_1  | mname_2  | is_equal  |\n",
    "|  3 | lname_1  | lname_2  | jaro_wink |\n",
    "|  4 | lname_1  | lname_2  | dlev_len  |\n",
    "|  5 | phone_1  | phone_2  | dlev_len  |\n",
    "|  6 | add_1    | add_2    | jaro_wink |\n",
    "|  7 | add_1    | add_2    | dlev_len  |\n",
    "|  8 | county_1 | county_2 | dlev_len  |\n",
    "|  9 | state_1  | state_2  | is_equal  |\n",
    "| 10 | zip_1    | zip_2    | dlev_len  |\n",
    "| 11 | sex_1    | sex_2    | is_equal  |\n",
    "| 12 | email_1  | email_2  | jaro_wink |\n",
    "\n",
    "* Compare 'fname' fields using Jaro-Winkler similarity. Missing values are handled by returning -1.0.\n",
    "* Add a custom Damerau-Levenshtein distance comparison for 'fname' fields. The result is stored in a new comparison feature named 'dlev_fname'.\n",
    "* Compare 'mname' fields using exact match. Missing values are handled by returning -1.0.\n",
    "* Compare 'lname' fields using Jaro-Winkler similarity. Missing values are handled by returning -1.0.\n",
    "* Add a custom Damerau-Levenshtein distance comparison for 'lname' fields. The result is stored in a new comparison feature named 'dlev_lname'.\n",
    "* Add a custom Damerau-Levenshtein distance comparison for 'phone' fields. The result is stored in a new comparison feature named 'dlev_phone'.\n",
    "* Compare 'add' fields using Jaro-Winkler similarity. Missing values are handled by returning -1.0.\n",
    "* Add a custom Damerau-Levenshtein distance comparison for 'add' fields. The result is stored in a new comparison feature named 'dlev_add'.\n",
    "* Add a custom Damerau-Levenshtein distance comparison for 'county' fields. The result is stored in a new comparison feature named 'dlev_county'.\n",
    "* Compare 'state' fields using exact match. Missing values are handled by returning -1.0.\n",
    "* Add a custom Damerau-Levenshtein distance comparison for 'zip' fields. The result is stored in a new comparison feature named 'dlev_zip'.\n",
    "* Compare 'sex' fields using exact match. Missing values are handled by returning -1.0.\n",
    "* Compare 'email' fields using Jaro-Winkler similarity. Missing values are handled by returning -1.0.\n",
    "* Add a custom Damerau-Levenshtein distance comparison for 'email' fields. The result is stored in a new comparison feature named 'dlev_email'.\n",
    "* Compare whether 'fname' and 'lname' fields are swapped between two records. The result is stored in a new comparison feature named 'swapped_fname_lname'.\n",
    "* Compare 'dob' fields using a custom date of birth comparison function. The result is stored in a new comparison feature named 'dob_comparison_score'.\n",
    "* Compare 'fname', 'sdx_fname', and 'meta_fname' fields using a custom name comparison function. The result is stored in a new comparison feature named 'fname_comparison_score'.\n",
    "* Compare 'lname', 'sdx_lname', and 'meta_lname' fields using a custom name comparison function. The result is stored in a new comparison feature named 'lname_comparison_score'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Compare object\n",
    "compare = rl.Compare()\n",
    "\n",
    "# Perform all of the comparisons described above\n",
    "compare.string('fname','fname','jarowinkler', label='jwink_fname', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('fname','fname',label='dlev_fname'))\n",
    "compare.exact('mname','mname', label='exact_mname',missing_value=-1.0)\n",
    "compare.string('lname','lname','jarowinkler',label='jwink_lname', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('lname','lname',label='dlev_lname'))\n",
    "compare.add(compare_custom_dlev_distance('phone','phone',label='dlev_phone'))\n",
    "compare.string('add','add','jarowinkler',label='jwink_add', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('add','add',label='dlev_add'))\n",
    "compare.add(compare_custom_dlev_distance('county','county',label='dlev_county'))\n",
    "compare.exact('state','state',label='exact_state', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('zip','zip',label='dlev_zip'))\n",
    "compare.exact('sex','sex', label='exact_sex',missing_value=-1.0)\n",
    "compare.string('email','email','jarowinkler',label='jwink_email', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('email','email',label='dlev_email'))\n",
    "compare.add(compare_swapped_fields(('fname','lname'),('fname','lname'),label='swapped_fname_lname'))\n",
    "compare.add(compare_dob_score('dob','dob',label='dob_comparison_score'))\n",
    "compare.add(compare_name_score(('fname','sdx_fname','meta_fname'),('fname','sdx_fname','meta_fname'),label='fname_comparison_score'))\n",
    "compare.add(compare_name_score(('lname','sdx_lname','meta_lname'),('lname','sdx_lname','meta_lname'),label='lname_comparison_score'))\n",
    "\n",
    "# Add our label column\n",
    "compare.exact('ssn','ssn',label='label')\n",
    "\n",
    "# Perform the comparisons (should take nearly 5 minutes)\n",
    "features = compare.compute(pairs,df1,df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Split by label\n",
    "\n",
    "The label=0 class has far more pairs than the label=1 class.\n",
    "We'll treat them differently when sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = features[features.label==0]\n",
    "df_minority = features[features.label==1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sample \"majority\" class\n",
    "\n",
    "As described above, the label=0 class has the majority of the pairs we've created.\n",
    "We want to sample them in a non-random way.\n",
    "If we were to sample the label=0 data randomly and select 5 rows, we'd see something like this (exaggerated for sample's sake):\n",
    "\n",
    "| fname_comparison_score | dlev_fname | jwink_lname | dob_comparison_score | dlev_phone |\n",
    "|-----------------------|------------|-------------|---------------------|------------|\n",
    "|         low              |      low      |   low          |    medium                 |    low        |\n",
    "|         low              |   low         |    low         |      low               |    medium        |\n",
    "|         low              |    low        |    low         |      low               |    low        |\n",
    "|         low              |    low        |    low         |      medium               |    low        |\n",
    "|         low              |    low        |    low         |      low               |    low        |\n",
    "\n",
    "You'll notice that most of the scores are \"low\".  This is because most of the generated pairs are very easy to determine as non-matches.\n",
    "We're interested in finding the trickier situations for our model training.  That way, it can deal with tricky situations like twins, married people, and passed-down names.\n",
    "\n",
    "To do this, we divide each of the five columns listed above into \"low\",\"medium\",and \"high\" buckets to reflect their scores relative to the rest of the dataset.\n",
    "Let's consider these as 0,1,2 for this example sake.\n",
    "All buckets being low would be reflected as 0_0_0_0_0.\n",
    "All buckets being high would be reflected as 2_2_2_2_2.\n",
    "The way that the code is working is such that every combination between all zeros and all twos is sampled from equally.\n",
    "This way, we get a very non-random sample that contains a wide variety of combinations.\n",
    "\n",
    "#### Note: The following cell can take around 40 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins = pd.DataFrame()\n",
    "\n",
    "# Discretize the fields into bins\n",
    "for col in ['fname_comparison_score','dlev_fname', 'jwink_lname', 'dob_comparison_score', 'dlev_phone']:\n",
    "    df_bins[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n",
    "\n",
    "# Create a 'strata' column that combines the bins\n",
    "df_bins['strata'] = df_bins.apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "\n",
    "# Combine the bins back to df_majority\n",
    "df_majority = pd.concat([df_majority, df_bins], axis=1)\n",
    "\n",
    "# Sample from each stratum\n",
    "samples = []\n",
    "for stratum, group in df_majority.groupby('strata'):\n",
    "    samples.append(group.sample(min(len(group), 100000 // df_majority['strata'].nunique()), random_state=42, replace=False))\n",
    "\n",
    "# Concatenate the samples into a single dataframe\n",
    "df_majority_sampled = pd.concat(samples)\n",
    "\n",
    "# Drop the bin and strata columns\n",
    "df_majority_sampled = df_majority_sampled.drop(['fname_comparison_score_bin', 'dlev_fname_bin', 'jwink_lname_bin', 'dob_comparison_score_bin', 'dlev_phone_bin', 'strata'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sample the minority class (randomly)\n",
    "\n",
    "We're less concerned about specifically sampling this minority class.  So we'll do it randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample minority class so it matches in size with newly-sampled majority class \n",
    "df_minority_sampled = df_minority.sample(len(df_majority_sampled), random_state=42, replace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Combine minority and majority samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine majority class with upsampled minority class\n",
    "df_resampled = pd.concat([df_majority_sampled, df_minority_sampled])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to a .csv file in our data folder\n",
    "df_resampled.to_csv('./Data/advanced_synthetic_training_data.csv',header=True,index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
