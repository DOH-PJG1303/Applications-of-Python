{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Set\n",
    "---------\n",
    "\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-05-16  \n",
    "**Last Updated:** 2023-05-16  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "This notebook should serve to prepare our training data that we will use to train/test a machine learning model for record linkage.\n",
    "\n",
    "### Notes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data analysis tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Record linkage specific resources\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean, phonetic\n",
    "from recordlinkage.index import Block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Data\n",
    "\n",
    "Here, we're using synthetic data that I created to replicate Oregon's population.\n",
    "The data represents the population of individuals in the years 2020-2022 who were born in Coos County in Oregon.\n",
    "They may live elsewhere.\n",
    "Interesting features that this training data includes:\n",
    "* several people can live in the same building\n",
    "* families exist.  If a child is <18, they live with their parent or parents\n",
    "* fields change dependent on the year. If someone gets married and changes their name in 2020, their data in 2019 will look different than their data in 2020 for the lname field.\n",
    "\n",
    "For more information, reach out to PJ and he'd be happy to elaborate.\n",
    "Again, this data is synthetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Data/synthetic_df1.csv', dtype=str)[['ssn','fname','lname','dob','phone','add','unique_id','parents_partnership_id','house_id','building_id']]\n",
    "df2 = pd.read_csv('Data/synthetic_df2.csv', dtype=str)[['ssn','fname','lname','dob','phone','add','unique_id','parents_partnership_id','house_id','building_id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Record Linkage Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "for col in ['fname', 'lname', 'dob', 'phone', 'add']:\n",
    "    df1[col] = clean(df1[col])\n",
    "    df2[col] = clean(df2[col])\n",
    "\n",
    "# Generate metaphone versions of the fields\n",
    "for col in ['fname', 'lname']:\n",
    "    df1['meta_'+col] = phonetic(df1[col], method='metaphone')\n",
    "    df2['meta_'+col] = phonetic(df2[col], method='metaphone')\n",
    "\n",
    "# Create the index (pairs of records to compare)\n",
    "indexer = rl.Index()\n",
    "\n",
    "# Generate a blocking scheme as a union of the following blocks\n",
    "indexer.add(Block('dob'))\n",
    "indexer.add(Block(['meta_fname', 'meta_lname']))\n",
    "indexer.add(Block('building_id'))\n",
    "indexer.add(Block('parents_partnership_id'))\n",
    "\n",
    "pairs = indexer.index(df1, df2)\n",
    "\n",
    "# Create the Compare object\n",
    "compare_precursor = rl.Compare()\n",
    "compare = rl.Compare()\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "# Calculate the average similarity score for each field and use it as the missing value\n",
    "for col in ['fname', 'lname', 'dob', 'add']:\n",
    "    compare_precursor.string(col, col, method='jarowinkler', missing_value=-1, label=col)\n",
    "\n",
    "# Compute the comparison scores\n",
    "features_precursor = compare_precursor.compute(pairs, df1, df2)\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "# Calculate the average similarity score for each field\n",
    "for col in ['fname', 'lname', 'dob', 'add']:\n",
    "    average_score = features_precursor.replace(-1,None)[col].mean()\n",
    "    compare.string(col, col, method='jarowinkler', missing_value=average_score, label=col)\n",
    "\n",
    "# Compare the phone fields using damerau_levenshtein similarity\n",
    "compare.string('phone', 'phone', method='damerau_levenshtein', label='phone')\n",
    "compare.exact('ssn','ssn',label='label')\n",
    "\n",
    "# Compute the comparison scores\n",
    "features = compare.compute(pairs, df1, df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Preprocessing Step\n",
    "\n",
    "Right now, there are far more pairs with a `label=0` than `label=1`.\n",
    "Roughly 86.5% of the data has a `label=1`.\n",
    "\n",
    "If we used this output to train our model, we could get a model that predicts a label of 0 for every record.\n",
    "It would still have an accuracy score of 86.5%, which is misleading.\n",
    "A model that predicts 0 for everything is essentially useless.\n",
    "\n",
    "We'll format the data so that it has a roughly even number of each label class.\n",
    "We use some semi-complex stratification in order to sample our `label=0` class to contain records that are \"interesting\".\n",
    "If we were to randomly sample this majority label-class, we'd likely get nearly all instances where the two records are VERY dissimilar.\n",
    "By sampling in a non-random way, we can find \"interesting\" `label=0` pairs, where for instance firstname, lasname, dob, and address are all very similar, but they still represent different people."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Split label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = features[features.label==0]\n",
    "df_minority = features[features.label==1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Non-randomly sample majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjg1303\\AppData\\Local\\Temp\\ipykernel_4700\\2990472032.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_majority[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n",
      "C:\\Users\\pjg1303\\AppData\\Local\\Temp\\ipykernel_4700\\2990472032.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_majority[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n",
      "C:\\Users\\pjg1303\\AppData\\Local\\Temp\\ipykernel_4700\\2990472032.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_majority[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n",
      "C:\\Users\\pjg1303\\AppData\\Local\\Temp\\ipykernel_4700\\2990472032.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_majority[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n",
      "C:\\Users\\pjg1303\\AppData\\Local\\Temp\\ipykernel_4700\\2990472032.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_majority[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n"
     ]
    }
   ],
   "source": [
    "# Discretize the fields into bins\n",
    "for col in ['fname', 'lname', 'dob', 'phone', 'add']:\n",
    "    df_majority[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n",
    "\n",
    "# Create a 'strata' column that combines the bins\n",
    "df_majority['strata'] = df_majority[['fname_bin', 'lname_bin', 'dob_bin', 'phone_bin', 'add_bin']].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "\n",
    "# Sample from each stratum\n",
    "samples = []\n",
    "for stratum, group in df_majority.groupby('strata'):\n",
    "    samples.append(group.sample(min(len(group), 100000 // df_majority['strata'].nunique()), random_state=42))\n",
    "\n",
    "# Concatenate the samples into a single dataframe\n",
    "df_majority_sampled = pd.concat(samples)\n",
    "\n",
    "# Drop the bin and strata columns\n",
    "df_majority_sampled = df_majority_sampled.drop(['fname_bin', 'lname_bin', 'dob_bin', 'phone_bin', 'add_bin', 'strata'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Randomly sample minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample minority class so it matches in size with newly-sampled majority class \n",
    "df_minority_sampled = df_minority.sample(len(df_majority_sampled), random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine majority class with upsampled minority class\n",
    "df_resampled = pd.concat([df_majority_sampled, df_minority_sampled])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to a .csv file in our data folder\n",
    "df_resampled.to_csv('./Data/synthetic_training_data.csv',header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
