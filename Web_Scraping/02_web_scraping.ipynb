{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "---------\n",
    "\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-05-16  \n",
    "**Last Updated:** 2023-05-19  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "This notebook should serve to educate newcomers to Python on web-scraping.\n",
    "In this script, we'll cover the following popular web-scraping libraries:\n",
    "- requests\n",
    "- selenium\n",
    "\n",
    "### Notes\n",
    "The Selenium package is not native when you install python.\n",
    "See the selenium_setup.md file within this project sub-folder for instructions on how to get set up with python and selenium.\n",
    "\n",
    "Chat GPT was used to help create some of the documentation and code behind this script.\n",
    "Credit to this tool for being great at summarizing information and aiding in the learning process for myself and others.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Requests and BeautifulSoup\n",
    "\n",
    "\n",
    "The [Python Requests library](https://requests.readthedocs.io/en/latest/) is a popular HTTP client library that allows you to send HTTP requests using Python. With it, you can send HTTP/1.1 requests and handle the responses. This includes making HTTP requests (GET, POST, PUT, DELETE, etc.), handling query parameters, form-encoded data, files, and JSON data. The library abstracts the complexities of making requests behind a beautiful, simple API. With Requests, you can also handle cookies, sessions, and headers, all while providing thread-safety and connection pooling, making it a robust and efficient solution for interacting with web services.\n",
    "\n",
    "[BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) is a Python library designed for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner. BeautifulSoup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree. It sits on top of an HTML or XML parser and provides Python-friendly representations of the parse tree, making it easier for users to parse an HTML document and extract the information they need. It automatically converts incoming documents to Unicode and outgoing documents to UTF-8, making it highly reliable for web scraping and data extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define URL\n",
    "url = \"https://www.ssa.gov/oact/STATS/table4c6.html\"\n",
    "\n",
    "# Send HTTP request to URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print out our response\n",
    "print(response)\n",
    "\n",
    "# Parse HTML response\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table on the webpage - we're assuming here that the first table is the one we want to extract.  Python is a 0-indexed language\n",
    "table = soup.find_all('table')[0] \n",
    "\n",
    "# Parse the table into a pandas DataFrame with a MultiIndex header.  The first 2 rows represent the information we'd care about at the column-level\n",
    "df = pd.read_html(str(table), header=[0, 1])[0]\n",
    "\n",
    "# Drop the last row\n",
    "df = df.drop(df.tail(1).index)\n",
    "\n",
    "# Print the DataFrame\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "\n",
    "[Selenium](https://selenium-python.readthedocs.io/) is a powerful tool for controlling web browsers through programs and automating browser tasks. It works across different browsers and platforms and is a key tool for tasks such as web scraping, web testing, and automating repetitive tasks on the web. Selenium supports Python and offers the WebDriver API, which uses browser-native commands to provide a more realistic user experience when interacting with websites during testing. Selenium's ability to integrate with various programming languages, its compatibility with different operating systems, and its support for mobile testing make it a versatile choice for web-based application testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Read in our file path to the chromedriver\n",
    "with open('../secret_webdriverpath.pkl', 'rb') as file:\n",
    "    fpath = pickle.load(file)\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(fpath)\n",
    "\n",
    "# Allow some time to look at proper window (drag for presentation)\n",
    "time.sleep(3)\n",
    "\n",
    "# Go to www.google.com\n",
    "driver.get('https://www.google.com')\n",
    "\n",
    "# Find the search bar\n",
    "search_bar = driver.find_element(By.NAME, 'q')\n",
    "\n",
    "# Type 'ssa actuarial tables' and hit Enter\n",
    "search_bar.send_keys('ssa actuarial tables')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# # Wait for the search results to load\n",
    "# WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div#rso div.g')))\n",
    "\n",
    "# Click on the first search result link\n",
    "driver.find_element(By.CSS_SELECTOR, 'div#rso div.g a').click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Identify the underlying html of the page\n",
    "page_html = driver.page_source\n",
    "\n",
    "# Close the driver\n",
    "driver.close()\n",
    "\n",
    "################################################\n",
    "# Below should look familiar....\n",
    "################################################\n",
    "\n",
    "# Find the table on the webpage - we're assuming here that the first table is the one we want to extract.  Python is a 0-indexed language\n",
    "table = soup.find_all('table')[0] \n",
    "\n",
    "# Parse the table into a pandas DataFrame with a MultiIndex header.  The first 2 rows represent the information we'd care about at the column-level\n",
    "df = pd.read_html(str(table), header=[0, 1])[0]\n",
    "\n",
    "# Drop the last row\n",
    "df = df.drop(df.tail(1).index)\n",
    "\n",
    "# Print the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
